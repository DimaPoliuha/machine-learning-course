{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "- [Videos on Udemy](https://www.udemy.com/machinelearning/learn/lecture/10459594)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Now I am getting angry and I want my damn pho.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Honeslty it didn't taste THAT fresh.)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The potatoes were like rubber and you could te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The fries were great too.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A great touch.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1\n",
       "5     Now I am getting angry and I want my damn pho.      0\n",
       "6              Honeslty it didn't taste THAT fresh.)      0\n",
       "7  The potatoes were like rubber and you could te...      0\n",
       "8                          The fries were great too.      1\n",
       "9                                     A great touch.      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing additional Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dmitry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "for item in dataset['Review']:\n",
    "    review = re.sub(r'[^a-zA-Z]', ' ', item)\n",
    "    review = review.lower()\n",
    "    review = [ps.stem(word) for word in review.split(' ') if word and word not in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow love place',\n",
       " 'crust good',\n",
       " 'tasti textur nasti',\n",
       " 'stop late may bank holiday rick steve recommend love',\n",
       " 'select menu great price',\n",
       " 'get angri want damn pho',\n",
       " 'honeslti tast fresh',\n",
       " 'potato like rubber could tell made ahead time kept warmer',\n",
       " 'fri great',\n",
       " 'great touch']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset into Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73\n",
      "Precision: 0.5670103092783505\n",
      "Recall: 0.8208955223880597\n",
      "F1 Score: 0.6707317073170731\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "naive_bayes = {}\n",
    "naive_bayes['name'] = 'Naive Bayes'\n",
    "naive_bayes['classifier'] = GaussianNB()\n",
    "naive_bayes['classifier'].fit(X_train, y_train)\n",
    "\n",
    "naive_bayes['y_pred'] = naive_bayes['classifier'].predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "naive_bayes['confusion_matrix'] = confusion_matrix(y_test, naive_bayes['y_pred'])\n",
    "\n",
    "naive_bayes['tp'] = naive_bayes['confusion_matrix'][0, 0]\n",
    "naive_bayes['fp'] = naive_bayes['confusion_matrix'][0, 1]\n",
    "naive_bayes['fn'] = naive_bayes['confusion_matrix'][1, 0]\n",
    "naive_bayes['tn'] = naive_bayes['confusion_matrix'][1, 1]\n",
    "\n",
    "naive_bayes['accuracy'] = (naive_bayes['tp'] + naive_bayes['tn']) / (naive_bayes['tp'] + naive_bayes['tn'] + naive_bayes['fp'] + naive_bayes['fn'])\n",
    "naive_bayes['precision'] = naive_bayes['tp'] / (naive_bayes['tp'] + naive_bayes['fp'])\n",
    "naive_bayes['recall'] = naive_bayes['tp'] / (naive_bayes['tp'] + naive_bayes['fn'])\n",
    "naive_bayes['f1_score'] = 2 * naive_bayes['precision'] * naive_bayes['recall'] / (naive_bayes['precision'] + naive_bayes['recall'])\n",
    "\n",
    "print('Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nF1 Score: {}'.format(naive_bayes['accuracy'],\n",
    "                                                                     naive_bayes['precision'],\n",
    "                                                                     naive_bayes['recall'],\n",
    "                                                                     naive_bayes['f1_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "Precision: 0.7628865979381443\n",
      "Recall: 0.6788990825688074\n",
      "F1 Score: 0.7184466019417477\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = {}\n",
    "decision_tree['name'] = 'Decision Tree'\n",
    "decision_tree['classifier'] = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "decision_tree['classifier'].fit(X_train, y_train)\n",
    "\n",
    "decision_tree['y_pred'] = decision_tree['classifier'].predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "decision_tree['confusion_matrix'] = confusion_matrix(y_test, decision_tree['y_pred'])\n",
    "\n",
    "decision_tree['tp'] = decision_tree['confusion_matrix'][0, 0]\n",
    "decision_tree['fp'] = decision_tree['confusion_matrix'][0, 1]\n",
    "decision_tree['fn'] = decision_tree['confusion_matrix'][1, 0]\n",
    "decision_tree['tn'] = decision_tree['confusion_matrix'][1, 1]\n",
    "\n",
    "decision_tree['accuracy'] = (decision_tree['tp'] + decision_tree['tn']) / (decision_tree['tp'] + decision_tree['tn'] + decision_tree['fp'] + decision_tree['fn'])\n",
    "decision_tree['precision'] = decision_tree['tp'] / (decision_tree['tp'] + decision_tree['fp'])\n",
    "decision_tree['recall'] = decision_tree['tp'] / (decision_tree['tp'] + decision_tree['fn'])\n",
    "decision_tree['f1_score'] = 2 * decision_tree['precision'] * decision_tree['recall'] / (decision_tree['precision'] + decision_tree['recall'])\n",
    "\n",
    "print('Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nF1 Score: {}'.format(decision_tree['accuracy'],\n",
    "                                                                     decision_tree['precision'],\n",
    "                                                                     decision_tree['recall'],\n",
    "                                                                     decision_tree['f1_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n",
      "Precision: 0.8969072164948454\n",
      "Recall: 0.6541353383458647\n",
      "F1 Score: 0.7565217391304349\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = {}\n",
    "random_forest['name'] = 'Random Forest'\n",
    "random_forest['classifier'] = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)\n",
    "random_forest['classifier'].fit(X_train, y_train)\n",
    "\n",
    "random_forest['y_pred'] = random_forest['classifier'].predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "random_forest['confusion_matrix'] = confusion_matrix(y_test, random_forest['y_pred'])\n",
    "\n",
    "random_forest['tp'] = random_forest['confusion_matrix'][0, 0]\n",
    "random_forest['fp'] = random_forest['confusion_matrix'][0, 1]\n",
    "random_forest['fn'] = random_forest['confusion_matrix'][1, 0]\n",
    "random_forest['tn'] = random_forest['confusion_matrix'][1, 1]\n",
    "\n",
    "random_forest['accuracy'] = (random_forest['tp'] + random_forest['tn']) / (random_forest['tp'] + random_forest['tn'] + random_forest['fp'] + random_forest['fn'])\n",
    "random_forest['precision'] = random_forest['tp'] / (random_forest['tp'] + random_forest['fp'])\n",
    "random_forest['recall'] = random_forest['tp'] / (random_forest['tp'] + random_forest['fn'])\n",
    "random_forest['f1_score'] = 2 * random_forest['precision'] * random_forest['recall'] / (random_forest['precision'] + random_forest['recall'])\n",
    "\n",
    "print('Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nF1 Score: {}'.format(random_forest['accuracy'],\n",
    "                                                                     random_forest['precision'],\n",
    "                                                                     random_forest['recall'],\n",
    "                                                                     random_forest['f1_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n",
      "Precision: 0.7628865979381443\n",
      "Recall: 0.6915887850467289\n",
      "F1 Score: 0.7254901960784315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = {}\n",
    "svm['name'] = 'SVM'\n",
    "svm['classifier'] = SVC(kernel='linear', random_state=0)\n",
    "svm['classifier'].fit(X_train, y_train)\n",
    "\n",
    "svm['y_pred'] = svm['classifier'].predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "svm['confusion_matrix'] = confusion_matrix(y_test, svm['y_pred'])\n",
    "\n",
    "svm['tp'] = svm['confusion_matrix'][0, 0]\n",
    "svm['fp'] = svm['confusion_matrix'][0, 1]\n",
    "svm['fn'] = svm['confusion_matrix'][1, 0]\n",
    "svm['tn'] = svm['confusion_matrix'][1, 1]\n",
    "\n",
    "svm['accuracy'] = (svm['tp'] + svm['tn']) / (svm['tp'] + svm['tn'] + svm['fp'] + svm['fn'])\n",
    "svm['precision'] = svm['tp'] / (svm['tp'] + svm['fp'])\n",
    "svm['recall'] = svm['tp'] / (svm['tp'] + svm['fn'])\n",
    "svm['f1_score'] = 2 * svm['precision'] * svm['recall'] / (svm['precision'] + svm['recall'])\n",
    "\n",
    "print('Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nF1 Score: {}'.format(svm['accuracy'],\n",
    "                                                                     svm['precision'],\n",
    "                                                                     svm['recall'],\n",
    "                                                                     svm['f1_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.485\n",
      "Precision: 1.0\n",
      "Recall: 0.485\n",
      "F1 Score: 0.6531986531986532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "kernel_svm = {}\n",
    "kernel_svm['name'] = 'Kernel SVM'\n",
    "kernel_svm['classifier'] = SVC(kernel='rbf', random_state=0)\n",
    "kernel_svm['classifier'].fit(X_train, y_train)\n",
    "\n",
    "kernel_svm['y_pred'] = kernel_svm['classifier'].predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "kernel_svm['confusion_matrix'] = confusion_matrix(y_test, kernel_svm['y_pred'])\n",
    "\n",
    "kernel_svm['tp'] = kernel_svm['confusion_matrix'][0, 0]\n",
    "kernel_svm['fp'] = kernel_svm['confusion_matrix'][0, 1]\n",
    "kernel_svm['fn'] = kernel_svm['confusion_matrix'][1, 0]\n",
    "kernel_svm['tn'] = kernel_svm['confusion_matrix'][1, 1]\n",
    "\n",
    "kernel_svm['accuracy'] = (kernel_svm['tp'] + kernel_svm['tn']) / (kernel_svm['tp'] + kernel_svm['tn'] + kernel_svm['fp'] + kernel_svm['fn'])\n",
    "kernel_svm['precision'] = kernel_svm['tp'] / (kernel_svm['tp'] + kernel_svm['fp'])\n",
    "kernel_svm['recall'] = kernel_svm['tp'] / (kernel_svm['tp'] + kernel_svm['fn'])\n",
    "kernel_svm['f1_score'] = 2 * kernel_svm['precision'] * kernel_svm['recall'] / (kernel_svm['precision'] + kernel_svm['recall'])\n",
    "\n",
    "print('Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nF1 Score: {}'.format(kernel_svm['accuracy'],\n",
    "                                                                     kernel_svm['precision'],\n",
    "                                                                     kernel_svm['recall'],\n",
    "                                                                     kernel_svm['f1_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "Precision: 0.7835051546391752\n",
      "Recall: 0.672566371681416\n",
      "F1 Score: 0.7238095238095238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression = {}\n",
    "logistic_regression['name'] = 'Logistic Regression'\n",
    "logistic_regression['classifier'] = LogisticRegression(random_state=0)\n",
    "logistic_regression['classifier'].fit(X_train, y_train)\n",
    "\n",
    "logistic_regression['y_pred'] = logistic_regression['classifier'].predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "logistic_regression['confusion_matrix'] = confusion_matrix(y_test, logistic_regression['y_pred'])\n",
    "\n",
    "logistic_regression['tp'] = logistic_regression['confusion_matrix'][0, 0]\n",
    "logistic_regression['fp'] = logistic_regression['confusion_matrix'][0, 1]\n",
    "logistic_regression['fn'] = logistic_regression['confusion_matrix'][1, 0]\n",
    "logistic_regression['tn'] = logistic_regression['confusion_matrix'][1, 1]\n",
    "\n",
    "logistic_regression['accuracy'] = (logistic_regression['tp'] + logistic_regression['tn']) / (logistic_regression['tp'] + logistic_regression['tn'] + logistic_regression['fp'] + logistic_regression['fn'])\n",
    "logistic_regression['precision'] = logistic_regression['tp'] / (logistic_regression['tp'] + logistic_regression['fp'])\n",
    "logistic_regression['recall'] = logistic_regression['tp'] / (logistic_regression['tp'] + logistic_regression['fn'])\n",
    "logistic_regression['f1_score'] = 2 * logistic_regression['precision'] * logistic_regression['recall'] / (logistic_regression['precision'] + logistic_regression['recall'])\n",
    "\n",
    "print('Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nF1 Score: {}'.format(logistic_regression['accuracy'],\n",
    "                                                                     logistic_regression['precision'],\n",
    "                                                                     logistic_regression['recall'],\n",
    "                                                                     logistic_regression['f1_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61\n",
      "Precision: 0.7628865979381443\n",
      "Recall: 0.5736434108527132\n",
      "F1 Score: 0.654867256637168\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = {}\n",
    "knn['name'] = 'KNN'\n",
    "knn['classifier'] = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "knn['classifier'].fit(X_train, y_train)\n",
    "\n",
    "knn['y_pred'] = knn['classifier'].predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn['confusion_matrix'] = confusion_matrix(y_test, knn['y_pred'])\n",
    "\n",
    "knn['tp'] = knn['confusion_matrix'][0, 0]\n",
    "knn['fp'] = knn['confusion_matrix'][0, 1]\n",
    "knn['fn'] = knn['confusion_matrix'][1, 0]\n",
    "knn['tn'] = knn['confusion_matrix'][1, 1]\n",
    "\n",
    "knn['accuracy'] = (knn['tp'] + knn['tn']) / (knn['tp'] + knn['tn'] + knn['fp'] + knn['fn'])\n",
    "knn['precision'] = knn['tp'] / (knn['tp'] + knn['fp'])\n",
    "knn['recall'] = knn['tp'] / (knn['tp'] + knn['fn'])\n",
    "knn['f1_score'] = 2 * knn['precision'] * knn['recall'] / (knn['precision'] + knn['recall'])\n",
    "\n",
    "print('Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nF1 Score: {}'.format(knn['accuracy'],\n",
    "                                                                     knn['precision'],\n",
    "                                                                     knn['recall'],\n",
    "                                                                     knn['f1_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.730</td>\n",
       "      <td>0.567010</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.670732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.710</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>0.678899</td>\n",
       "      <td>0.718447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.720</td>\n",
       "      <td>0.896907</td>\n",
       "      <td>0.654135</td>\n",
       "      <td>0.756522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.720</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>0.691589</td>\n",
       "      <td>0.725490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kernel SVM</th>\n",
       "      <td>0.485</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.653199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.710</td>\n",
       "      <td>0.783505</td>\n",
       "      <td>0.672566</td>\n",
       "      <td>0.723810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.610</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>0.573643</td>\n",
       "      <td>0.654867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy  Precision    Recall  F1 Score\n",
       "Naive Bayes             0.730   0.567010  0.820896  0.670732\n",
       "Decision Tree           0.710   0.762887  0.678899  0.718447\n",
       "Random Forest           0.720   0.896907  0.654135  0.756522\n",
       "SVM                     0.720   0.762887  0.691589  0.725490\n",
       "Kernel SVM              0.485   1.000000  0.485000  0.653199\n",
       "Logistic Regression     0.710   0.783505  0.672566  0.723810\n",
       "KNN                     0.610   0.762887  0.573643  0.654867"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers = [naive_bayes, decision_tree, random_forest, svm, kernel_svm, logistic_regression, knn]\n",
    "\n",
    "pd.DataFrame([[classifier['accuracy'], classifier['precision'], classifier['recall'], classifier['f1_score']] for classifier in classifiers],\n",
    "             [classifier['name'] for classifier in classifiers],\n",
    "             ['Accuracy', 'Precision', 'Recall', 'F1 Score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
